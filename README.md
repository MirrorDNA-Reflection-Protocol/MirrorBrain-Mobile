# âŸ¡ MirrorBrain Mobile

**Sovereign AI on your phone.** Local LLM inference using llama.rn.

## Features
- ðŸ§  Local inference via llama.rn (llama.cpp React Native bindings)
- ðŸ“± Works offline â€” no cloud required
- ðŸ”’ Private â€” your data stays on device
- âš¡ Fast â€” optimized for mobile hardware

## Quick Start

```bash
# Install dependencies
npm install

# Run on Android (Pixel connected via USB)
npx react-native run-android

# Run on iOS
cd ios && pod install && cd ..
npx react-native run-ios
```

## Requirements
- Node.js 18+
- React Native CLI
- Android Studio (for Android)
- Xcode (for iOS)
- Java 17 (configured in `android/gradle.properties`)

## Model
The app downloads and runs [TinyLlama](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF) or similar quantized models locally.

## Project Structure
```
src/
â”œâ”€â”€ screens/        # App screens (Chat, Settings)
â”œâ”€â”€ services/       # LLM service, Kiwix service
â”œâ”€â”€ components/     # UI components
â””â”€â”€ utils/          # Helpers
```

## Part of MirrorDNA
This is the mobile component of the MirrorDNA Sovereign Stack:
- **MirrorBrain-Setup** â€” Mac Mini (desktop)
- **MirrorBrain-Mobile** â€” Android/iOS (this repo)
- **Mirror Intelligence** â€” Web (brief.activemirror.ai)
